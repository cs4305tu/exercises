{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4305TU: Week 6 - Artificial Neural Network - Lecture 2\n",
    "**7 October 2021**\n",
    "\n",
    "- Sander van Cranenburgh ([S.vanCranenburgh@tudelft.nl]())\n",
    "- Francisco Garrido-Valenzuela ([F.O.GarridoValenzuela@tudelft.nl]())\n",
    "\n",
    "**This notebook will not be graded. You do not have to submit the notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to set up your environment based on which platform you would like to use. In this case we offer two options:\n",
    "\n",
    "- Google Colaboratory (Colab)\n",
    "- Jupyter Lab or Notebooks (Local)\n",
    "\n",
    "### Using Colab\n",
    "\n",
    "Students using **Colab**, are ready to work! :)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you are using Google Colab (keep the exclamation mark)\n",
    "#!pip install biogeme\n",
    "#!git clone https://github.com/cs4305tu/exercises\n",
    "#root = 'exercises/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using local environment\n",
    "\n",
    "Students using their *local environments*, need to install all the dependencies used in this *Week 6*, to ensure compatibility, they also need to check the versions of each dependency. All dependencies are contained in the text file: **requirements.txt**. Just run the following notebook cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you are using your local environment (keep the exclamation mark)\n",
    "#!pip3 install -r requirements.txt\n",
    "#root = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "## Exercise 3: The effect of the number of nodes and the learning rate on the model performance and training of ANNs\n",
    "\n",
    "**Objectives:** \n",
    "* Understand the effects of number of nodes on the model performance\n",
    "* Understand the the effect of the learning rate on training time and model convergence\n",
    "* Apply Tensor Flow 2 to create and train more advanced ANN typologies\n",
    "\n",
    "### i. import Python packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Using tensorflow \",tf.__version__)\n",
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.layers import Conv2D, Add, Reshape\n",
    "from keras.models import Model\n",
    "from keras.utils.np_utils import to_categorical   \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import ML packaged and modules\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Load the data and create a dataframe \n",
    "Note that we use the samedata set as in Lecture 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data\n",
    "df = pd.read_csv(f'{root}datasets/dcm/smartphone_choicedata2021.csv', sep='\\t')\n",
    "\n",
    "# Define the features\n",
    "X = df[['COST_1','SIZE_1','STORAGE_1','CAM_1','COST_2','SIZE_2','STORAGE_2','CAM_2','COST_3','SIZE_3','STORAGE_3','CAM_3','GENDER','INC']]\n",
    "\n",
    "# Define the output target\n",
    "Y = df['CHOICE']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Split the data in a training set and a test set and rescale the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in a training set and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33)\n",
    "\n",
    "# Define scaler \n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "\n",
    "# Apply scaler to both the training and test set\n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. Investigate the effect of the number of nodes on the model performance\n",
    "To do so, we use the same ANN as in Exercise 2. But, we train this ANN using a different number of nodes in the hidden layers: [1,2,3,5,7,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model for using it in the for loop\n",
    "def new_ann_sk(num_nodes = 5, epoch = 400):\n",
    "\n",
    "    # Buid the ANN\n",
    "    layers = (num_nodes,num_nodes) # Here we use a network with two hidden layers, with *num_nodes* each \n",
    "\n",
    "    # Create the ANNs (MultiLayerPerceptron aka MLP)\n",
    "    mlp = MLPClassifier(solver = 'adam', alpha = 1e-5, hidden_layer_sizes = layers, max_iter = 1000) \n",
    "\n",
    "    # Train the ANN. Note that sklearn does not permit anything else than batch gradient descent\n",
    "    mlp.fit(X_train, Y_train)\n",
    "\n",
    "    # Use the trained ANN to make predictions for the test data\n",
    "    probs_ANN_X_test = mlp.predict_proba(X_test)\n",
    "\n",
    "    # Compute the prediction performance, based on the cross-entropy (aka log_loss). A lower cross-entropy signals a better model.\n",
    "    cross_entropy_ANN = log_loss(Y_test, probs_ANN_X_test)\n",
    "    #print(f'The cross-entropy on the test data of the ANN with {num_nodes} is {cross_entropy_ANN}')\n",
    "    return cross_entropy_ANN\n",
    "\n",
    "\n",
    "# Train the model using a different number of nodes and collec the cross-entropy for each typology\n",
    "nodes_range = [1,2,3,5,7,10]\n",
    "cross_val = []\n",
    "for nneurons in nodes_range:\n",
    "    cross_val.append(new_ann_sk(num_nodes = nneurons, epoch = 400))\n",
    "\n",
    "# Plot cross entropy as a function the number of nodes in the (two) hidden layers\n",
    "plt.subplot(111)\n",
    "plt.plot(nodes_range,cross_val, color = 'blue', label = 'test')\n",
    "plt.xlabel('Number of nodes at the hidden layers')\n",
    "plt.ylabel('Cross entropy')\n",
    "plt.ylim(0.4, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1. Regarding the effect of the nodes:\n",
    "- #### (a) What is the 'optimal number of nodes' for these data set?\n",
    "- #### (b) Why does adding more nodes lead to a better model performance (up until some point)?\n",
    "- #### (c) Why does the model performance not further improve when adding more nodes after say 10?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "\n",
    "**ANSWER** <br>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v. Rebuild the ANN in TensorFlow\n",
    "Next, we rebuild the ANN of SKLearn in TensorFlow (TF). TF allows for building more advanced networks and for more custom tweaking and tuning. We need TF to be able to build the Hybrid ANNs in exercise 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Convert output labels to categoricals\n",
    "TF requires the output labels the be categorical for classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_c = to_categorical(Y_train-1, num_classes = 3)\n",
    "Y_test_c = to_categorical(Y_test-1, num_classes = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Building the ANN structure\n",
    "\n",
    "In TF there are various ways to build a network. Here we use Keras **Functional API**. The Keras functional API is a way to create graph neural networks that are more flexible than using a **Sequential API**. The Functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs.\n",
    "\n",
    "- [Functional API documentation](https://www.tensorflow.org/guide/keras/functional)\n",
    "- [Sequential API documentation](https://www.tensorflow.org/guide/keras/sequential_model)\n",
    "\n",
    "We build the layers one by one and then create the model by pulling these together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input layer\n",
    "X = Input((X_train.shape[1]), name ='features')\n",
    "\n",
    "# Create the hidden layers\n",
    "num_nodes = 5 # Number of nodes per hidden layer\n",
    "layer1 = Dense(units = num_nodes, name = \"layer1\", use_bias = True)(X)           \n",
    "layer2 = Dense(units = num_nodes, name = \"layer2\", use_bias = True)(layer1)\n",
    "\n",
    "# Create the output layer \n",
    "Y = Dense(units = Y_train_c.shape[1], activation='softmax', name = \"output\")(layer2) \n",
    "\n",
    "# Create the model by indicating the input and outputs in the graph of layers\n",
    "model = Model(inputs = X, outputs = Y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vi. Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model, passing the traning configuration\n",
    "model.compile(optimizer = Adam(learning_rate = 1e-2), metrics = [\"accuracy\"], loss = 'categorical_crossentropy')\n",
    "\n",
    "# Early stopping stops the training once the performance on the test set no longer improves\n",
    "early_stopping = EarlyStopping(patience = 3, monitor = 'val_loss')\n",
    "\n",
    "# Train the model, while keeping track of the performance at each epoch\n",
    "history = model.fit(X_train,Y_train_c, batch_size=len(X_train), epochs = 1000, verbose = 1, validation_data = (X_test, Y_test_c), callbacks = [early_stopping])\n",
    "\n",
    "print('The cross-entropy on the test data of the tensor flow ANN is',\"{:.3f}\".format(history.history['loss'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss as a function of epochs\n",
    "plt.subplot(211)\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.plot(history.history['loss'],     color = 'blue', label = 'train')\n",
    "plt.plot(history.history['val_loss'], color = 'red', label = 'test')\n",
    "plt.ylim(0.4,1)\n",
    "\n",
    "# plot accuracy\n",
    "plt.subplot(212)\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history.history['accuracy'],     color = 'blue', label = 'train')\n",
    "plt.plot(history.history['val_accuracy'], color = 'red', label = 'test')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "\n",
    "# Tweak spacing between subplots to prevent labels from overlapping\n",
    "plt.subplots_adjust(hspace = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 2. Probably the model performance varies between the TF and SKlearn implementation of the ANN. Why do we not find the exact same results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "\n",
    "**ANSWER** <br>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 3. Try different values for the \"learning rate\" to investigate its effect on the training time and model convergence/performance. What do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color='red'>\n",
    "\n",
    "**ANSWER**\n",
    "    \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "## **Exercise 4: Build the Hybrid RUM-MNL-ANN model**\n",
    "\n",
    "**Objective:**  See how by combining ANNs and DCMs the best of both worlds, in terms of model peformance and interpretability can be attained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Declare properties of the data set and training settings \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NALT = 3           # Number of alterantives in the data set.\n",
    "no_X_MNL = 2       # Number of attributes with behavioural interest (-->MNL model part).  In this example we are particularly interested in the WtP for extra storage space --> Cost & Storage\n",
    "no_X_ANN = 8       # Number of features without behavioural interest (-->ANN model part). In this example we are not behaviourall interested in Camera, Size, and the socio demographic variables\n",
    "num_nodes = 5      # Number of nodes in hidden layer(s). Again we use 2 hidden layers with *num_nodes* nodes each\n",
    "nEpoch = 500       # Number epochs for training (max). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Create the layers of the MNL part of the hybrid model\n",
    "\n",
    "Using the structure of TF Functional API. The MNL part is made as follow:\n",
    "\n",
    "<img src=\"https://github.com/cs4305tu/week6/blob/main/MNLpart.png?raw=true\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT FOR MNL PART\n",
    "X_MNL = Input((no_X_MNL, NALT,1), name = 'Features2MNL')\n",
    "\n",
    "# COMPUTE UTILITY FOR MNL\n",
    "V_MNL = Conv2D(filters = 1, kernel_size = [no_X_MNL,1], strides = (1,1), padding = 'valid', name = 'MNL_layer', use_bias = False, trainable = True)(X_MNL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Create the layers of the ANN part of the hybrid model\n",
    "\n",
    "Using the structure of TF Functional API. The ANN part is made as follow:\n",
    "\n",
    "<img src=\"https://github.com/cs4305tu/week6/blob/main/ANNpart.png?raw=true\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT FOR ANN PART\n",
    "X_ANN = Input((no_X_ANN), name ='Features2ANN')\n",
    "\n",
    "# CREATE HIDDEN LAYER(S) OF ANN\n",
    "layer1_ANN = Dense(units = num_nodes, name = \"ANN_layer1\", use_bias = True)(X_ANN) \n",
    "layer2_ANN = Dense(units = num_nodes, name = \"ANN_layer2\", use_bias = True)(layer1_ANN)\n",
    "\n",
    "# COMPUTE UTILITY FOR ANN \n",
    "V_ANN = Dense(units = NALT, name = \"V_ANN\")(layer2_ANN) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. Combine the MNL and ANN parts and build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESHAPE TENSORS TO [1 X NALT]\n",
    "V_MNL = Reshape([NALT], name = 'Flatten_Dim_MNL')(V_MNL)\n",
    "V_ANN = Reshape([NALT], name = 'Flatten_Dim_ANN')(V_ANN) \n",
    "\n",
    "# SUM THE UTILITIES OF BOTH MODEL PARTS\n",
    "V_MNL_ANN = Add(name = \"Combining_Vs\")([V_MNL,V_ANN])\n",
    "\n",
    "# CREATE LOGIT (AKA SOFTMAX ) OUTPUT LAYER\n",
    "logits = Activation('softmax', name = 'Probability')(V_MNL_ANN)\n",
    "\n",
    "# BUILD THE MODEL\n",
    "model = Model(inputs = [X_MNL, X_ANN], outputs = logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v. Define the input and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and the output class\n",
    "X = df[['COST_1','SIZE_1','STORAGE_1','CAM_1','COST_2','SIZE_2','STORAGE_2','CAM_2','COST_3','SIZE_3','STORAGE_3','CAM_3','GENDER','INC']]\n",
    "\n",
    "# Define the output target\n",
    "Y = df['CHOICE']\n",
    "Y_cat = to_categorical(Y-1, num_classes = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vi. Create input variables for both parts of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x input for MNL layer, and rescale\n",
    "scale = 100 # We cannot just use the sklearn scaler here, as it is import for the interpretation later how the input data are scaled. \n",
    "\n",
    "x_mnl = np.array([[np.divide(X['COST_1'], scale), np.divide(X['STORAGE_1'], scale)],\n",
    "                  [np.divide(X['COST_2'], scale), np.divide(X['STORAGE_2'], scale)],\n",
    "                  [np.divide(X['COST_3'], scale), np.divide(X['STORAGE_3'], scale)]])\n",
    "x_mnl = np.swapaxes(x_mnl, 0, 2)\n",
    "x_mnl = np.expand_dims(x_mnl, 3)\n",
    "print('Shape of x_mnl', x_mnl.shape)\n",
    "\n",
    "# Create x input for ANN layer\n",
    "x_ann = np.array([[X['SIZE_1'], X['CAM_1'], X['SIZE_2'], X['CAM_2'], X['SIZE_3'], X['CAM_3'], X['GENDER'], X['INC']]])\n",
    "x_ann = np.squeeze(np.swapaxes(x_ann, 0, 2))\n",
    "\n",
    "# Rescale input for the ANN part\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(x_ann)  \n",
    "x_ann = scaler.transform(x_ann)  \n",
    "print('Shape of x_ann',x_ann.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vii. Split the data in a training set and a test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training and test part\n",
    "X_mnl_train, X_mnl_test, Y_train, Y_test = train_test_split(x_mnl, Y_cat, random_state = 1, test_size = 0.35)\n",
    "X_ann_train, X_ann_test, Y_train, Y_test = train_test_split(x_ann, Y_cat, random_state = 1, test_size = 0.35)\n",
    "print('Total number of obervations in the data set = ', len(x_mnl))\n",
    "print('Number of obervations in the training set   = ', len(X_mnl_train))\n",
    "print('Number of obervations in the test set       = ', len(X_mnl_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### viii. Compile the hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer = Adam(learning_rate = 1e-2), metrics = [\"accuracy\"], loss = 'categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ix. Train the hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "early_stopping = EarlyStopping(patience = 4, monitor = 'val_loss')\n",
    "history = model.fit([X_mnl_train, X_ann_train],Y_train, batch_size=len(X_mnl_train), epochs = nEpoch, verbose = 1, validation_data = ([X_mnl_test, X_ann_test], Y_test), callbacks = [early_stopping])\n",
    "\n",
    "betas_layer = model.get_layer(name = 'MNL_layer')\n",
    "betas = betas_layer.get_weights()\n",
    "print('The cross-entropy on the test data of the tensor flow ANN is',\"{:.3f}\".format(history.history['loss'][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss as a function of epochs\n",
    "plt.subplot(211)\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.plot(history.history['loss'],     color = 'blue', label = 'train')\n",
    "plt.plot(history.history['val_loss'], color = 'red', label = 'test')\n",
    "plt.ylim(0.4,1)\n",
    "\n",
    "# plot accuracy\n",
    "plt.subplot(212)\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history.history['accuracy'],     color = 'blue', label = 'train')\n",
    "plt.plot(history.history['val_accuracy'], color = 'red', label = 'test')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "\n",
    "# Tweak spacing between subplots to prevent labels from overlapping\n",
    "plt.subplots_adjust(hspace = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x. Obtain behavioural insights into the Willingness-to-Pay for storage space:\n",
    "- (1) Compute WtP for extra storage space from MNL part of the model\n",
    "- (2) Derive insights into the importance of extra storage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Compute WtP for extra storage from MNL part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the trained taste parameters, from the MNL part\n",
    "beta_COST = np.squeeze((betas[0][0]))\n",
    "beta_STORAGE = np.squeeze((betas[0][1]))\n",
    "print('Beta_COST    = ', \"{:.3f}\".format(beta_COST )) \n",
    "print('Beta_STORAGE = ', \"{:.3f}\".format(beta_STORAGE ))\n",
    "\n",
    "# Compute the Willingness to Pay for a Gb extra storage space\n",
    "WtP_storage = -beta_STORAGE/beta_COST\n",
    "print('WtP storage  = â‚¬', \"{:.2f}\".format(WtP_storage),' per extra Gigabite')\n",
    "print()\n",
    "\n",
    "# For comparison convert the loss to log-likelihood and rho^2\n",
    "hist_loss_train = history.history.get('loss')\n",
    "LL_final_train = -np.array(hist_loss_train[len(hist_loss_train)-1]) *len(Y_train)\n",
    "print('Cross-entropy training data at final epoch  = ', \"{:.3f}\".format(hist_loss_train[len(hist_loss_train)-1]))\n",
    "print('Log-likelihood training data at final epoch = ', \"{:.1f}\".format(LL_final_train))\n",
    "print('rho square training data at final epoch     = ', \"{:.2f}\".format(1 - LL_final_train / -(len(Y_train)*np.log(3))))\n",
    "print()\n",
    "\n",
    "hist_loss_test = history.history.get('val_loss')\n",
    "LL_final_test = -np.array(hist_loss_test[len(hist_loss_test)-1]) *len(Y_test)\n",
    "print('Cross-entropy test data at final epoch     = ', \"{:.3f}\".format(hist_loss_test[len(hist_loss_test)-1]))\n",
    "print('Log-likelihood test data at final epoch    = ', \"{:.1f}\".format(LL_final_test))\n",
    "print('rho square test data at final epoch        = ', \"{:.2f}\".format(1 - LL_final_test / -(len(Y_test)*np.log(3))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Derive insights into the importance of extra storage from ANN part of the model by re-doing the analysis we did in lecture 1.<br>\n",
    "\n",
    "\n",
    "That is, let's simulate the effect of a (change in) attribute levels of STORAGE on the choice probabilities.\n",
    "\n",
    "To do so, we take the following steps:\n",
    " 1. Create an ndarray containing the following alternatives. Use Male (GENDER=0) and Income level 2 (INC=2)for the socio-demographic variables\n",
    " 2. Apply the trained ANN to these data points. \n",
    " 3. Plot the effect of the STORAGE of alternative 3 on the choice probabilities that alternatives 1,2, 3 are choosen for a Male with Income level 2.\n",
    "\n",
    "\n",
    "| COST_1 | SIZE_1 | STORAGE_1 | CAM_1 | COST_2 | SIZE_2 | STORAGE_2 | CAM_2 | COST_3 | SIZE_3 | STORAGE_3 | CAM_3 |\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| 500 | 6.0 | 64 | 2 | 500 | 6.0 | 64 | 2 | 500 | 6.0 | **32**  | 2 |\n",
    "| 500 | 6.0 | 64 | 2 | 500 | 6.0 | 64 | 2 | 500 | 6.0 | **64**  | 2 |\n",
    "| 500 | 6.0 | 64 | 2 | 500 | 6.0 | 64 | 2 | 500 | 6.0 | **128** | 2 |\n",
    "| 500 | 6.0 | 64 | 2 | 500 | 6.0 | 64 | 2 | 500 | 6.0 | **256** | 2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create an ndarray containing the alternatives in the table above\n",
    "X_sim = np.array([[500,6.0,64,2,500,6.0,64,2,500,6.0,32,2,0,2],[500,6.0,64,2,500,6.0,64,2,500,6.0,64,2,0,2],[500,6.0,64,2,500,6.0,64,2,500,6.0,128,2,0,2],[500,6.0,64,2,500,6.0,64,2,500,6.0,256,2,0,2]])\n",
    "\n",
    "# Step 2: Apply the trained ANN to these data points\n",
    "# Rescale input data using the same scaler as used for training\n",
    "# MNL input\n",
    "scale = 100\n",
    "x_mnl_sim = np.array([[np.divide(X_sim[:,0], scale), np.divide(X_sim[:,2], scale)],\n",
    "                      [np.divide(X_sim[:,4], scale), np.divide(X_sim[:,6], scale)],\n",
    "                      [np.divide(X_sim[:,8], scale), np.divide(X_sim[:,10], scale)]])\n",
    "x_mnl_sim = np.swapaxes(x_mnl_sim, 0, 2)\n",
    "x_mnl_sim = np.expand_dims(x_mnl_sim, 3)\n",
    "print('Shape of x_mnl', x_mnl_sim.shape)\n",
    "\n",
    "# ANN input\n",
    "x_ann_sim = np.array([[X_sim[:,1], X_sim[:,3], X_sim[:,5], X_sim[:,7], X_sim[:,9], X_sim[:,11], np.zeros(4), 2*np.ones(4)]])\n",
    "x_ann_sim = np.squeeze(np.swapaxes(x_ann_sim, 0, 2))\n",
    "x_ann_sim = scaler.transform(x_ann_sim)  \n",
    "print('Shape of x_ann', x_ann_sim.shape)\n",
    "\n",
    "# Simulate the choice probabilities by applying the trained model\n",
    "probs_x_sim = model.predict([x_mnl_sim, x_ann_sim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Plot the effect of the STORAGE of alternative 3 on the choice probabilities\n",
    "# Plot the outputs\n",
    "fig=plt.figure(figsize=(12,8), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(221)\n",
    "x_levels = np.array([32, 64, 128, 256])\n",
    "plt.plot(x_levels,probs_x_sim[:,0], label = \"Probability alternative 1\")\n",
    "plt.plot(x_levels,probs_x_sim[:,1], label = \"Probability alternative 2\")\n",
    "plt.plot(x_levels,probs_x_sim[:,2], label = \"Probability alternative 3\")\n",
    "plt.legend(loc = \"best\")\n",
    "plt.title('Hybrid model')\n",
    "plt.xlabel(\"Storage space [Gb]\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlim([0, 260])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.subplot(223)\n",
    "utility_ANN_X_sim_V1 = 0 # As utilities are not absolute, we can arbitrarily fix V1 to zero\n",
    "utility_ANN_X_sim_V2 = np.log(probs_x_sim[:,1]/probs_x_sim[:,0]) # Utility of alternative 2 relative to V1\n",
    "utility_ANN_X_sim_V3 = np.log(probs_x_sim[:,2]/probs_x_sim[:,0]) # Utility of alternative 3 relative to V1\n",
    "plt.plot(x_levels,np.tile(utility_ANN_X_sim_V1,(len(x_levels))), label = \"Utility alternative 1\")\n",
    "plt.plot(x_levels,utility_ANN_X_sim_V2,                          label = \"Utility alternative 2\")\n",
    "plt.plot(x_levels,utility_ANN_X_sim_V3,                          label = \"Utility alternative 3\")\n",
    "plt.legend(loc = \"best\")\n",
    "plt.title('Hybrid model')\n",
    "plt.xlabel(\"Storage space [Gb]\")\n",
    "plt.ylabel(\"Utility\")\n",
    "plt.xlim([0,260])\n",
    "\n",
    "# For comparison we also add the plots for the DCM\n",
    "betas_DCM = {\"B_camera\": 0.812309,\"B_cost\":-0.010879,\"B_size\":2.106680,\"B_storage\":0.004567} # These are the beta estimates of exercise 1\n",
    "\n",
    "# Compute utility using the estimated betas\n",
    "V1_X_sim = betas_DCM['B_cost']*X_sim[:,0] + betas_DCM['B_size']*X_sim[:,1] + betas_DCM['B_storage']*X_sim[:,2] + betas_DCM['B_camera']*X_sim[:,3]\n",
    "V2_X_sim = betas_DCM['B_cost']*X_sim[:,4] + betas_DCM['B_size']*X_sim[:,5] + betas_DCM['B_storage']*X_sim[:,6] + betas_DCM['B_camera']*X_sim[:,7]\n",
    "V3_X_sim = betas_DCM['B_cost']*X_sim[:,8] + betas_DCM['B_size']*X_sim[:,9] + betas_DCM['B_storage']*X_sim[:,10] + betas_DCM['B_camera']*X_sim[:,11]\n",
    "\n",
    "# Compute probabilities\n",
    "P1_sim = np.exp(V1_X_sim) / (np.exp(V1_X_sim)+np.exp(V2_X_sim)+np.exp(V3_X_sim))\n",
    "P2_sim = np.exp(V2_X_sim) / (np.exp(V1_X_sim)+np.exp(V2_X_sim)+np.exp(V3_X_sim))\n",
    "P3_sim = np.exp(V3_X_sim) / (np.exp(V1_X_sim)+np.exp(V2_X_sim)+np.exp(V3_X_sim))\n",
    "P_sim = np.transpose([P1_sim, P2_sim, P3_sim])\n",
    "\n",
    "plt.subplot(222)\n",
    "x_levels = np.array([32, 64, 128, 256])\n",
    "plt.plot(x_levels,P_sim[:,0], label= \"Probability alternative 1\")\n",
    "plt.plot(x_levels,P_sim[:,1], label= \"Probability alternative 2\")\n",
    "plt.plot(x_levels,P_sim[:,2], label= \"Probability alternative 3\")\n",
    "plt.title('Discrete choice model exercise 1')\n",
    "plt.legend(loc = \"best\")\n",
    "plt.xlabel(\"Storage space [Gb]\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlim([0,260])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.subplot(224)\n",
    "x_levels = np.array([32, 64, 128, 256])\n",
    "plt.plot(x_levels,V1_X_sim-V1_X_sim, label= \"Utility alternative 1\")\n",
    "plt.plot(x_levels,V2_X_sim-V1_X_sim, label= \"Utility alternative 2\")\n",
    "plt.plot(x_levels,V3_X_sim-V1_X_sim, label= \"Utility alternative 3\")\n",
    "plt.title('Discrete choice model exercise 1')\n",
    "plt.legend(loc = \"best\")\n",
    "plt.xlabel(\"Storage space [Gb]\")\n",
    "plt.ylabel(\"Utility\")\n",
    "plt.xlim([0,260])\n",
    "\n",
    "# Tweak spacing between subplots to prevent labels from overlapping\n",
    "plt.subplots_adjust(hspace = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1. Compare these plots with those found in exercise 2 (lecture 1). What catches the eye? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "\n",
    "**ANSWER** <br>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 2. Compare the model performance, the estimates, and no. weights between this hybrid model, and those of the DCM and ANN of lecture 1. Explain why these results are in line with expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color='red'>\n",
    "\n",
    "**ANSWER** <br>\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "026be6e5a1123b745c834c4f20bb4e6186274d8f23878cd8715984c0247c49fb"
  },
  "kernelspec": {
   "display_name": "Python3.9 (uyui)",
   "language": "python",
   "name": "uyui"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
